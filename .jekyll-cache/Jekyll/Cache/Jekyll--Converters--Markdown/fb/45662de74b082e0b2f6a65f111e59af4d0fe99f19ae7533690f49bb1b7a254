I" ?<p>The gradient descent algorithm and its variants are some of the most widely used optimization algorithms in machine learning today. In this post a super simple example of gradient descent will be implemented.<!--more--></p>

<h2 id="example">Example</h2>

<p>We will use the simple function $L(x)=x^2$, and call it our <em>loss function</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># The maximum and minimum value of the function.
</span><span class="n">M</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># The loss function.
</span><span class="n">L</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">M</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">L</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss function'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/2019-02-27-gradient-descent_files/2019-02-27-gradient-descent_3_0.png" alt="png" /></p>

<h2 id="algorithm">Algorithm</h2>

<p>The derivative of $L(\theta)=\theta^2$ wrt. x is $\frac{\partial}{\partial \theta}L(\theta)=2\theta$. The gradient descent algorithm goes as follow:</p>

<ol>
  <li>
    <p><strong>Initiate</strong> $\theta_0\in\mathbb{R}, \eta\in\mathbb{R}$.</p>

    <p>1.1. <strong>Update</strong>: $\theta_{t+1} = \theta_t - \eta \frac{\partial}{\partial \theta} L(\theta_t)$.</p>

    <p>1.2. <strong>Stop</strong>: If stopping criterium is satisfied.</p>
  </li>
</ol>

<p>Intuitively one takes a small step in the direction of steepest local descent. Lets implement some helper functions to simulate the algorithm.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">simulate_gradient_descent</span><span class="p">(</span><span class="n">theta_0</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s">"""Simulate the next gradient by running the algorithm.
    
    Args:
        theta_0 (float): The initial value of theta.
        eta (float): The learning rate.
        max_iter (int): The maximum number of iterations.
    
    Returns:
        List[float]: A list of thetas walked by the algorithm.
    """</span>

    <span class="c1"># Save the results in below
</span>    <span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># The derivative of x**2
</span>    <span class="n">dL</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>

    <span class="c1"># 1. Initiate
</span>    <span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">M</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>

        <span class="c1"># 1.1. Update
</span>        <span class="n">thetas</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">thetas</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dL</span><span class="p">(</span><span class="n">thetas</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">thetas</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">theta_path</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
    <span class="s">"""Plot the path walked by the algorithm.
    
    Args:
        theta_path (List[float]): A list of thetas walked by the algorithm.
        eta (float): The learning rate.
    """</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">L</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_path</span><span class="p">,</span> <span class="n">L</span><span class="p">(</span><span class="n">theta_path</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">eta</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">})</span>
</code></pre></div></div>

<h2 id="results">Results</h2>

<p>Lets plot the path of the gradient descent algorithm.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">theta_path</span> <span class="o">=</span> <span class="n">simulate_gradient_descent</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rc</span><span class="p">(</span><span class="s">'text'</span><span class="p">,</span> <span class="n">usetex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">theta_path</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">theta_path</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">L</span><span class="p">(</span><span class="n">theta_path</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="sa">r</span><span class="s">'$\theta_0$'</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">theta_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">L</span><span class="p">(</span><span class="n">theta_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="sa">r</span><span class="s">'$\theta_T$'</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/2019-02-27-gradient-descent_files/2019-02-27-gradient-descent_7_0.png" alt="png" /></p>

<p>It takes small steps with small $\eta$ and large steps with large $\eta$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">eta</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">theta_path</span> <span class="o">=</span> <span class="n">simulate_gradient_descent</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">theta_path</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/2019-02-27-gradient-descent_files/2019-02-27-gradient-descent_9_0.png" alt="png" /></p>

<p>With $\eta&gt;1$ it occilates to infinity.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">1.01</span>
<span class="n">theta_path</span> <span class="o">=</span> <span class="n">simulate_gradient_descent</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">theta_path</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">theta_path</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">L</span><span class="p">(</span><span class="n">theta_path</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="sa">r</span><span class="s">'$\theta_0$'</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">theta_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">L</span><span class="p">(</span><span class="n">theta_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="sa">r</span><span class="s">'$\theta_T$'</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/2019-02-27-gradient-descent_files/2019-02-27-gradient-descent_11_0.png" alt="png" /></p>

<h2 id="intuition">Intuition</h2>

<p>The intuition of gradient descent is that in each iteration of the <strong>update</strong> step one does a local linear approximation of the loss function, a 1st order Taylor expansion of $L(\theta+d)$ around $d=0$, that is for $d\in\mathbb{R}, \theta_t\in\mathbb{R}$</p>

\[\begin{align}
    L(\theta_t+d) &amp;\approx L(\theta_t) + \frac{\partial L(\theta_t+d)}{\partial d}(0) \cdot (d - 0) \\
    &amp;= L(\theta_t) + \frac{\partial (\theta_t + d)}{\partial d}(0) \frac{\partial L(x)}{\partial x}(\theta_t+0) \cdot (d - 0)  &amp;&amp; \text{Chain rule.} \\
    &amp;= L(\theta_t) + \frac{\partial L(x)}{\partial x} (\theta_t) \cdot d
\end{align}\]

<p>Using this local approximation one sees that $L$ is minimized in the direction of the negative gradient $d=-\frac{\partial L(x)}{\partial x} (\theta_t)$.</p>

<p>The reason why the algorithm has the stepsize $\eta$ is intuitvely because the local linear approximation only works well around a neighbourhood of $\theta_t$.</p>

<h2 id="higher-dimensions">Higher Dimensions</h2>

<p>This also holds in higher dimensions where the approximation is $d\in\mathbb{R^d}, \theta_t\in\mathbb{R^d}$</p>

\[\begin{align}
    L(\theta_t+d) \approx L(\theta_t) + \nabla L(\theta_t)^T d
\end{align}\]

<p>Restricting $\lVert d\rVert=1$ then one can see by Cauchy-Schwarz inequality that</p>

\[\begin{align}
    \nabla L(\theta_t)^T d &amp; \geq - \lvert \nabla L(\theta_t)^T d \rvert \\
    &amp; \geq - \lVert \nabla L(\theta_t) \rVert \lVert d \rVert &amp;&amp; \text{Cauchy Schwarz.} \\
        &amp;= \nabla L(\theta_t)^T\frac{- \nabla L(\theta_t)}{\lVert\nabla L(\theta_t)\rVert}
\end{align}\]

<p>So $\nabla L(\theta_t)^T d \geq \nabla L(\theta_t)^T d^\star$ where $d^\star=\frac{-\nabla L(\theta_t)}{\lVert\nabla L(\theta_t)\rVert}$, therefore $-\nabla L(\theta_t)$ is the local direction which minimizes the loss function.</p>

<p>Gradient descent applies this local approximation and moves along the negative gradient in each iteration.</p>

\[\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)\]

<p>The $\eta$ is often called the <em>learning rate</em>. It is a tuning parameter, that controls how far the algorithm steps along the negative gradient.</p>
:ET