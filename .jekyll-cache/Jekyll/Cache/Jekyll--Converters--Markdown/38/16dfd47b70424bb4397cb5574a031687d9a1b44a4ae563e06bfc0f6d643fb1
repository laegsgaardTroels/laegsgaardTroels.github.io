I"I^<p>In this post the famous AdaBoost algoritm will be implemented in R<!--more--> and tested on simulated data. The post is intented to yield some intuition and understanding about what boosting is via an exercise.</p>

<p>What is boosting? Lets start explaining this with a nice analogy provided by the masters of boosting: Yoav Freund and Robert E. Schapire, you could imagine the horse racing-gampler being an overly intelligent gambler from Peaky Blinders:</p>

<p>“… <em>A horse-racing gambler, hoping to maximize his winnings, decides to create a computer program that will accurately predict the winner of a horse race based on the usual information (number of races recently won by each horse, betting odds for each horse, etc.). To create such a program, he asks a highly successful expert gambler to explain his betting strategy. Not surprisingly, the expert is unable to articulate a grand set of rules for selecting a horse. On the other hand, when presented with the data for a specific set of races, the expert has no trouble coming up with a “rule of thumb” for that set of races (such as, “Bet on the horse that has recently won the most races” or “Bet on the horse with the most favored odds”). Although such a rule of thumb, by itself, is obviously very rough and inaccurate, it is not unreasonable to expect it to provide predictions that are at least a little bit better than random guessing. Furthermore, by repeatedly asking the expert’s opinion on
different collections of races, the gambler is able to extract many rules of thumb.
In order to use these rules of thumb to maximum advantage, there are two problems faced by the gambler: First, how should he choose the collections of races presented to the expert so as to extract rules of thumb from the expert that will be the most useful? Second, once he has collected many rules of thumb, how can they be combined into a single, highly accurate prediction rule?</em> … “
-<a href="https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">Yoav Freund and Robert E. Schapire</a></p>

<p>The idea of boosting is to take these weak rules of thumbs, also called <em>weak classifiers</em> \(h_t\), and via a procedure, called a <em>boosting algorithm</em>, produce a strong classifier.</p>

\[H_T(x)=\text{sign}\sum_{t=0}^T \alpha_t h_t(x)\]

<p>AdaBoost was the first really succesful boosting algorithm, it has undergone a lot of empirical testing and theoretical study. If you are more interested after this post, in the theory, then I suggest you look into the book [2].</p>

<h2 id="packages">Packages</h2>

<p>The packages that will be used in this post is the <code class="language-plaintext highlighter-rouge">ggplot2</code>, a package used for plotting in R, <code class="language-plaintext highlighter-rouge">latex2exp</code>, which is used to insert some latex code in R and <code class="language-plaintext highlighter-rouge">pacman</code> which has the function <code class="language-plaintext highlighter-rouge">pacman::p_load</code> that can be used to install and load packages,</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pacman</span><span class="o">::</span><span class="n">p_load</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">,</span><span class="w"> </span><span class="n">latex2exp</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">pacman::p_load</code> function can be seen as a more robust alternative to the functions <code class="language-plaintext highlighter-rouge">require</code> and <code class="language-plaintext highlighter-rouge">library</code>, which are usually used to load in packages.</p>

<h2 id="simulating-data">Simulating data</h2>

<p>To have complete control over the set-up for this exercise, the data for the classification problem will be simulated. First we will simulate <code class="language-plaintext highlighter-rouge">m &lt;- 10^5</code>  datapoints in a circle with added noise <code class="language-plaintext highlighter-rouge">sd&lt;-0.13</code>, and put the result in the dataframe <code class="language-plaintext highlighter-rouge">circ.df</code>, with the label <code class="language-plaintext highlighter-rouge">-1</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">runif</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="o">*</span><span class="nb">pi</span><span class="p">)</span><span class="w">
</span><span class="n">circ.x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span><span class="w">
</span><span class="n">circ.y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span><span class="w">
</span><span class="n">circ.df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">circ.x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">circ.y</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Simulate <code class="language-plaintext highlighter-rouge">m</code> points inside the circle with added noise and put it in <code class="language-plaintext highlighter-rouge">center.df</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">center.x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span><span class="w">
</span><span class="n">center.y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span><span class="w">
</span><span class="n">center.df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">center.x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">center.y</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Bind the two dataframes together row-wise with rbind and scrample the data s.t. they don’t lie in order.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df &lt;- rbind(circ.df, center.df)
df &lt;- df[sample(1:dim(df)[1],dim(df)[1]),]
</code></pre></div></div>

<p>Split it up in a training- and test- set, s.t. the model can be validated on the test-set afterwards</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="nf">round</span><span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">df</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="o">/</span><span class="m">2</span><span class="p">),]</span><span class="w">
</span><span class="n">test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df</span><span class="p">[</span><span class="nf">round</span><span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">df</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">:</span><span class="nf">dim</span><span class="p">(</span><span class="n">df</span><span class="p">)[</span><span class="m">1</span><span class="p">],]</span><span class="w">
</span><span class="n">train.X</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">train</span><span class="p">[,</span><span class="m">-1</span><span class="p">]</span><span class="w">
</span><span class="n">train.y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">train</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="n">test.X</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">test</span><span class="p">[,</span><span class="m">-1</span><span class="p">]</span><span class="w">
</span><span class="n">test.y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">test</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p>Finally plot the training data to see how it looks like. The blue points are those simulated on the circle, with label -1, and the red are the ones simulated inside the circle, label 1.</p>

<p><img src="/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoostExampleData.png" /></p>

<h2 id="make-weak-classifier">Make weak classifier</h2>

<p>The basic component of boosting is the <em>weak classifiers</em>, these are the rules of thumb in the above analogy. The AdaBoost algorithm combines these to a strong classifer.</p>

<p>Say we have some prior knowlede about the problem and decides to construct the weak classifiers as follows: First make a finite set of classifiers, which are plotted below, for each line below (vertical and horizontal) there are two hypotheses, one which is positive when points is on one side of the line, and one which is positive when point is on the other side. The classifier selected is then the one which minimizes the weigthed error:</p>

\[e_t=\sum_{i=0}^{2m} w_i 1\{h_t(x_i)\neq y_i\}\]

<p><img src="/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoostExampleHypothesisSet.png" /></p>

<p>In practice the weak classifiers are often simple decision trees, could be stumps: Trees with only one split. But in general one could try out the AdaBoost with almost any procedure in which one can weigh the error.</p>

<h2 id="the-adaboost-algorithm">The AdaBoost algorithm</h2>

<p>The algorithm keeps some quantities throughout a loop. These are here implemented below.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="n">nrow</span><span class="p">(</span><span class="n">train.X</span><span class="p">),</span><span class="n">nrow</span><span class="p">(</span><span class="n">train.X</span><span class="p">))</span><span class="w">
</span><span class="n">H</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">()</span><span class="w">
</span><span class="n">alphas</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<p>The w denotes the weights presented above. These can be viewed as a probability distribution held over, in each round <code class="language-plaintext highlighter-rouge">t</code> of AdaBoost, the observations, and initialized as uniform. This distribution is used to fit a weak classifier <code class="language-plaintext highlighter-rouge">h_t</code> in each iteration, and to get a weighted accuracy <code class="language-plaintext highlighter-rouge">e_t</code>.</p>

<p>Two additional quantities are also keept throughout the loops. The training error and test error:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_err</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span><span class="n">test_err</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<p>these will be used for evaluation of the algorithm afterwards.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">T_</span><span class="p">){</span><span class="w">

  </span><span class="n">res</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">fit</span><span class="p">(</span><span class="n">train.X</span><span class="p">,</span><span class="w"> </span><span class="n">train.y</span><span class="p">,</span><span class="w"> </span><span class="n">H.space</span><span class="p">,</span><span class="w"> </span><span class="n">w</span><span class="p">)</span><span class="w">

  </span><span class="n">e_t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">res</span><span class="p">[[</span><span class="m">1</span><span class="p">]]</span><span class="w">

  </span><span class="n">h_t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">res</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span><span class="w">

  </span><span class="n">alpha_t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">log</span><span class="p">((</span><span class="m">1</span><span class="o">-</span><span class="n">e_t</span><span class="p">)</span><span class="o">/</span><span class="n">e_t</span><span class="p">)</span><span class="w">

  </span><span class="n">Z_t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">  </span><span class="m">2</span><span class="o">*</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">e_t</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">e_t</span><span class="p">))</span><span class="w">

  </span><span class="n">w</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">alpha_t</span><span class="o">*</span><span class="n">train.y</span><span class="o">*</span><span class="n">predict</span><span class="p">(</span><span class="n">train.X</span><span class="p">,</span><span class="nf">list</span><span class="p">(</span><span class="n">h_t</span><span class="p">),</span><span class="m">1</span><span class="p">))</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Z_t</span><span class="w">

  </span><span class="n">alphas</span><span class="p">[[</span><span class="n">t</span><span class="p">]]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">alpha_t</span><span class="w">
  </span><span class="n">H</span><span class="p">[[</span><span class="n">t</span><span class="p">]]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h_t</span><span class="w">


  </span><span class="n">train_err</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">train_err</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">train.X</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">alphas</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">train.y</span><span class="p">))</span><span class="w">
  </span><span class="n">test_err</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">test_err</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">test.X</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">alphas</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">test.y</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>The weighted accuracy <code class="language-plaintext highlighter-rouge">e_t</code> is used to weigh the new weak hypothesis in the final ensemble by <code class="language-plaintext highlighter-rouge">alpha_t</code>, weighing them s.t. they greedly contributes the most to the ensemble in terms of in-sample error. This can also intuitively be inferred by a plot of the functional dependency between these two quantities</p>

<p>At the end of each iteration the distribution w is updated, putting more weight to incorrectly classified observations <code class="language-plaintext highlighter-rouge">w * exp(-alpha_t*train.y*predict(train.X,list(h_t),1))</code>. The <code class="language-plaintext highlighter-rouge">Z_t</code> is used for normalization of the weights to a probability distribution.</p>

<h2 id="running-the-algorithm">Running the algorithm</h2>

<p>Below one sees how the algorithm combines the weak classifiers. To the left one sees the combined classifier at iteration t and to the right the hypothesis fitted at iteration t of AdaBoost. Observations marked green is classified correctly by the given hypothesis and observations marked red are classified incorrectly. One observes how AdaBoost zooms in on the observations which is hard to classify, in the last iterations: Those on the inside of the circle. AdaBoost is also used in the outlier detection because of this property.</p>

<p>One also observes the algorithm sometimes chooses a hypothesis to the far right classifying all observations as the sam\e, which makes sense when the majority of the observations, in the weighted error at the given iteration, is of that label, as seen in iteration 3, it does seem to make sense in the ensemble to add this seemingly useless classifier.</p>

<div align="center">
	<div id="gifdiv">
	  <img src="/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoostExampleHypothesisAndH1-compressed.jpg" onclick="changeImage()" id="adagif" />
	</div>
 </div>
<script>
    var adagifIsJPG = true;
    (new Image()).src = "/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoost.gif";
    function changeImage() {
      if (adagifIsJPG) {
         document.getElementById("adagif").src = "/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoost.gif";
         adagifIsJPG = false;
      } else {
         document.getElementById("adagif").src = "/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoostExampleHypothesisAndH1-compressed.jpg";
         adagifIsJPG = true;
      }
    }
</script>

<h2 id="result-on-the-test-set">Result on the test set</h2>

<p>Below one sees the training- (blue) and test- (red) error, which seems to be almost identical in this example. The green line is plot off a probably almost correct (PAC) learning bound: A bound on how bad an algorithm can perform under some rather general statistical assumptions, with a given probability, hence probably almost. The below green line is a plot of this bound: Out-of-sample error is below this line with 95% probability.  The bound seems to predict a fast overfitting as a function of the number of trees, i.e. one observes that the bound increases very fast as a function of the number of trees in the ensemble, it cannot bound the generalization error.</p>

<p>But as seen in the plotted training- and test-error, red and blue, it seems to be benificial to run AdaBoost in more rounds than indicated by the bound, which is also the case in practical applications.</p>

<p><img src="/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoostExamplegeneralizationBound.png" /></p>

<p>One interesting aspect about these bounds on generalization performance from a practical point of view, is that they can often be decomposed into two terms,</p>

\[R(H) \leq \hat{R}(H)+\Omega(H)\]

<p>Where \(R(H)\) is the true generalization error, \(\hat{R}(H)\) is the in-sample loss, which decreases as a function of complexity, here being the number of trees in the ensemble, and  \(\Omega(H)\) is a complexity term, which increases as a complexity increases, here it increases as the number of trees grows. Such bounds, found many places in theory, indicates a trade-off between in-sample fit and complexity, a phenomenon also sees in practice.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This post touched the surface of some components of AdaBoost, and hopefully woke some interest into these types of algorithms.</p>

<h2 id="references">References</h2>

<p>[1] Yoav Freund and Robert E. Schapire, <em><a href="https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">A Short Introduction to Boosting</a></em>. Journal of Japanese Society for Artificial Intelligence, 1999</p>

<p>[2] Yoav Freund and Robert E. Schapire, <em>Boosting: Foundations and Algorithms</em>. The MIT Press, 2012</p>
:ET