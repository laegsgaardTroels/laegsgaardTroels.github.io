<!doctype html>
<html lang="en">
  <head>

      <style>
        img {
          max-width: 100%;
          margin: auto;
          display: block;
        }
        figcaption {
          max-width: 100%;
          margin: auto;
          display: block;
        }
        code {
          font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
          background-color: #f5f5f5;
          padding: .2em .4em;
          font-size: 85%;
          margin: 0;
        }
        pre {
          margin: 1em 0;
          background-color: #f5f5f5;
          padding: 1em;
          overflow: auto;
        }
        pre code {
          padding: 0;
          overflow: visible;
          overflow-wrap: normal;
        }
        .sourceCode {
         background-color: #f5f5f5;
         overflow: visible;
        }
        hr {
          background-color: #1a1a1a;
          border: none;
          height: 1px;
          margin: 1em 0;
        }
        table {
          margin: 1em 0;
          border-collapse: collapse;
          width: 100%;
          overflow-x: auto;
          display: block;
          font-variant-numeric: lining-nums tabular-nums;
        }
        table caption {
          margin-bottom: 0.75em;
        }
        tbody {
          margin-top: 0.5em;
          border-top: 1px solid #1a1a1a;
          border-bottom: 1px solid #1a1a1a;
        }
        th {
          border-top: 1px solid #1a1a1a;
          padding: 0.25em 0.5em 0.25em 0.5em;
        }
        td {
          padding: 0.125em 0.5em 0.25em 0.5em;
        }
        header {
          margin-bottom: 4em;
          text-align: center;
        }
        #TOC li {
          list-style: none;
        }
        #TOC ul {
          padding-left: 1.3em;
        }
        #TOC > ul {
          padding-left: 0;
        }
        #TOC a:not(:hover) {
          text-decoration: none;
        }
        code{white-space: pre-wrap;}
        span.smallcaps{font-variant: small-caps;}
        span.underline{text-decoration: underline;}
        div.column{display: inline-block; vertical-align: top; width: 50%;}
        div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
        ul.task-list{list-style: none;}
        pre > code.sourceCode { white-space: pre; position: relative; }
        pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
        pre > code.sourceCode > span:empty { height: 1.2em; }
        .sourceCode { overflow: visible; }
        code.sourceCode > span { color: inherit; text-decoration: inherit; }
        div.sourceCode { margin: 1em 0; }
        pre.sourceCode { margin: 0; }
        @media screen {
        div.sourceCode { overflow: auto; }
        }
        @media print {
        pre > code.sourceCode { white-space: pre-wrap; }
        pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
        }
        pre.numberSource code
          { counter-reset: source-line 0; }
        pre.numberSource code > span
          { position: relative; left: -4em; counter-increment: source-line; }
        pre.numberSource code > span > a:first-child::before
          { content: counter(source-line);
            position: relative; left: -1em; text-align: right; vertical-align: baseline;
            border: none; display: inline-block;
            -webkit-touch-callout: none; -webkit-user-select: none;
            -khtml-user-select: none; -moz-user-select: none;
            -ms-user-select: none; user-select: none;
            padding: 0 4px; width: 4em;
            color: #aaaaaa;
          }
        pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
        div.sourceCode
          {   }
        @media screen {
        pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
        }
        code span.al { color: #ff0000; font-weight: bold; } /* Alert */
        code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
        code span.at { color: #7d9029; } /* Attribute */
        code span.bn { color: #40a070; } /* BaseN */
        code span.bu { } /* BuiltIn */
        code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
        code span.ch { color: #4070a0; } /* Char */
        code span.cn { color: #880000; } /* Constant */
        code span.co { color: #60a0b0; font-style: italic; } /* Comment */
        code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
        code span.do { color: #ba2121; font-style: italic; } /* Documentation */
        code span.dt { color: #902000; } /* DataType */
        code span.dv { color: #40a070; } /* DecVal */
        code span.er { color: #ff0000; font-weight: bold; } /* Error */
        code span.ex { } /* Extension */
        code span.fl { color: #40a070; } /* Float */
        code span.fu { color: #06287e; } /* Function */
        code span.im { color: #007020; font-weight: bold; } /* Import */
        code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
        code span.kw { color: #007020; font-weight: bold; } /* Keyword */
        code span.op { color: #666666; } /* Operator */
        code span.ot { color: #007020; } /* Other */
        code span.pp { color: #bc7a00; } /* Preprocessor */
        code span.sc { color: #4070a0; } /* SpecialChar */
        code span.ss { color: #bb6688; } /* SpecialString */
        code span.st { color: #4070a0; } /* String */
        code span.va { color: #19177c; } /* Variable */
        code span.vs { color: #4070a0; } /* VerbatimString */
        code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
      </style>
              <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
      
                <!-- Using Bootstrap starter template: https://getbootstrap.com/docs/4.0/getting-started/introduction/-->
                <!-- Required meta tags -->
                <meta charset="utf-8">
                <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
                <link rel="shortcut icon" type="image/png" href="/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoostExampleData.png">

                <!-- <link rel="stylesheet" href="/assets/css/bootstrap.min.css">Bootstrap CSS -->


                <!-- 
                <script type="text/x-mathjax-config">
                    MathJax.Hub.Config({
                    jax: ["input/TeX", "output/HTML-CSS"],
                    tex2jax: {
                      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
                      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
                      processEscapes: true,
                      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                    }
                    //,
                    //displayAlign: "left",
                    //displayIndent: "2em"
                  });
                </script>
                <script type="text/javascript" async
                        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
                </script>
                MathJax -->

                <!-- Font Awesome CSS -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

                <!-- Disqus -->
                <script id="dsq-count-scr" src="//machinelearningnotes-1.disqus.com/count.js" async></script>
                      <script
                      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
                      type="text/javascript"></script>
      
    <title>Xgboost</title>
  </head>
  <body>

        <nav class="navbar navbar-expand-lg navbar-light bg-light">
            <a class="navbar-brand" href="/">ML-Notes</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
                <div class="navbar-nav">
                    <a class="nav-link" href="/index.html"> Blog </a>
                    <a class="nav-link" href="/courses.html"> Courses </a>
                    <a class="nav-link" href="/about.html"> About </a>
                </div>
            </div>
        </nav>

    
    <div class="container">
        <br>
        <div class="page-header">
            <div class="pull-left">
                <h1>Xgboost</h1>
                <strong>Algorithm</strong>
                <div class="text-muted">2017-11-06</div>
            </div>
            
            <div class="pull-right">

                
            </div>
            <div class="clearfix"></div>
        </div>
        

        <br>
        <p>In this post we will go through an example application of the
        XGBoost algorithm<!--more--> using data from a competition on
        the crowdsourced machine learning homepage
        <a src="https://www.kaggle.com">Kaggle</a>. We will show some of
        the model diagnostics one can make with XGBoost. One of these
        being the partial dependence plot, which is a general technique
        for analyzing black box methods like XGBoost.</p>
        <h2 id="data">Data</h2>
        <p>We will use the data from the Kaggle competition: “House
        Prices: Advanced Regression Techniques”, you can download it
        from this link:
        <a src= "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">Data</a>.
        We will only use the train.csv file. Remember to set the csv on
        your current working directory in R, you can use the
        <code>setwd(directory)</code> function in R, to set the working
        directory to the <code>directory</code> that contains the
        data.</p>
        <h2 id="packages">Packages</h2>
        <p>We will use the <code>readr</code>-package which we will use
        to read in the csv-file containing the data. The
        <code>xgboost</code>-package which contains the implementation
        of XGBoost in R. The <code>Matrix</code>-package which will be
        used to construct a sparse model matrix that we will feed to the
        XGBoost algorithm. The <code>DiagrammeR</code>-package and
        <code>igraph</code>-package which XGBoost uses for plotting and
        one of these plotting tools depend upon the package
        <code>Ckmeans.1d.dp</code>. The <code>ggplot2</code>-package for
        plotting and the <code>gridExtra</code>-package for plotting a
        grid in <code>ggplot2</code>.</p>
        <p>And finally the <code>dplyr</code> package, this package is
        purely a convinience package for data-manipulation in R, it
        contains the pipe operator <code>%&gt;%</code> which takes the
        output of the function to the left and feeds it to the first
        argument of the function to the right. Besides being very
        intuitive it also makes your code more readable.</p>
        <div class="sourceCode" id="cb1"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pacman<span class="sc">::</span><span class="fu">p_load</span>(readr, xgboost, Matrix, igraph, DiagrammeR, Ckmeans<span class="fl">.1</span>d.dp, ggplot, gridExtra, dplyr)</span></code></pre></div>
        <h2 id="read-data-and-formatting">Read data and formatting</h2>
        <p>Lets read in the data, put it in a <code>data.frame</code>
        and change character columns to factor columns, awesomely done
        using <code>dplyr</code>.</p>
        <div class="sourceCode" id="cb2"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;train.csv&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> <span class="fu">mutate_if</span>(is.character, factor)</span></code></pre></div>
        <p>Now lets prepare the data for the algorithm. First lets
        construct a matrix with the factors expanded using
        one-hot-encoding, and lets do expand it to a sparse matrix
        instead of a regular matrix by using
        <code>sparse.model.matrix</code> from the
        <code>Matrix</code>-package, this is better for memory
        consumption if it were a larger dataset.</p>
        <p>The <code>model.matrix</code> function in R is usually used
        for linear regression, R uses it to expand factors and
        interactions in the model formula out into dummies using the
        contrasts argument, the <code>sparse.model.matrix</code>
        function does the same thing but outputs a sparse matrix. Here
        we will use it for one-hot-encoding of the factors.</p>
        <div class="sourceCode" id="cb3"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>xg.data <span class="ot">&lt;-</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sparse.model.matrix</span>(<span class="sc">~</span> . <span class="sc">-</span><span class="dv">1</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">model.frame</span>(train[,<span class="sc">-</span><span class="fu">which</span>(<span class="fu">names</span>(train) <span class="sc">%in%</span> <span class="st">&quot;SalePrice&quot;</span>)],<span class="at">na.action =</span> na.pass),</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">contrasts.arg =</span> <span class="fu">lapply</span>(train[,<span class="fu">sapply</span>(train,is.factor)], contrasts, <span class="at">contrasts=</span><span class="cn">FALSE</span>))</span></code></pre></div>
        <p>Now we will put it in a data format for XGBoost.</p>
        <div class="sourceCode" id="cb4"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>xg.labelledData <span class="ot">=</span><span class="fu">xgb.DMatrix</span>(xg.data, <span class="at">label=</span>train<span class="sc">$</span>SalePrice)</span></code></pre></div>
        <h2 id="the-xgboost-model">The XGBoost model</h2>
        <p>This section is intended to give a super short intuitive
        introduction to XGBoost from a more mathematical point of
        view.</p>
        <p>The XGBoost model is a boosting model, it is a more advanced
        version of a normal gradient boosting model, which is fairly
        easy to understand. In normal gradient boosting the objective is
        to minimize the loss function:</p>
        <p><span class="math display">\[\mathcal{L}(F) :=  \frac{1}{N}
        \sum_{i=1}^{N} L(F(x_i),y_i)\]</span></p>
        <p>For the purpose of this minimization one is only interested
        in <span class="math inline">\(F\)</span> evaluated at <span
        class="math inline">\(F(x_1),...,F(x_N)\)</span> which one can
        regard as <span class="math inline">\(N\)</span> real variables:
        <span class="math display">\[f_i:=F(x_i)\in\mathbb{R}\]</span>
        Viewed this way the goal is now to minimize: <span
        class="math inline">\(\mathcal{L}(f_1,...,f_N) = \frac{1}{N}
        \sum_{i=1}^{N} L(f_i,y_i)\)</span>. So one observes that <span
        class="math inline">\(\mathcal{L}\)</span> can be thought of as
        a real-valued function defined on <span
        class="math inline">\(\mathbb{R}^N\)</span>, one which can be
        minimized by applying gradient descent i.e. initialize <span
        class="math inline">\(F=(f_1,...,f_N)&#39;=0\in\mathbb{R}^N\)</span>
        and update using <span class="math display">\[  F \gets F - \eta
        \nabla \mathcal{L}(F)\]</span> Where <span
        class="math inline">\(\nabla\mathcal{L}:=(\frac{\partial\mathcal{L}}{\partial
        f_1},...,\frac{\partial\mathcal{L}}{\partial f_N})&#39;\)</span>
        is the gradient evaluated in the current in-sample predictions
        <span class="math inline">\(F=(f_1,...,f_N)&#39;\)</span>.</p>
        <p>This procedure does not specify any meaningful prediction
        out-of-sample, because one is only minimizing <span
        class="math inline">\(\mathcal{L}\)</span> with respect to <span
        class="math inline">\((f_1,...,f_N)&#39;=(F(x_1),...,F(x_N))&#39;\)</span>,
        so this procedure only yields meaningful predictions on the
        training points i.e. the predictions <span
        class="math inline">\(F(x_1),...,F(x_N)\)</span> for the
        training points <span
        class="math inline">\(x_1,...,x_N\)</span>. To yield meaningful
        prediction for inputs which is not <span
        class="math inline">\(x_1,...,x_N\)</span> one imposes the
        restriction that each update must come from a class of base
        functions: <span class="math inline">\(\mathcal{H}\)</span>. One
        can now initialize <span
        class="math inline">\(F=0\in\mathbb{R}\)</span> and use the
        update rule <span class="math display">\[F \gets F - \eta
        h,\quad h\in\mathcal{H}\]</span> in each iteration of this
        analog of gradient descent to functional space. The <span
        class="math inline">\(h\)</span> is chosen to be the function
        closest to the gradient <span
        class="math inline">\(\nabla\mathcal{L}\)</span> with respect to
        the euclidian norm (the <span class="math inline">\(L^2\)</span>
        norm). Each of these base functions is then added to <span
        class="math inline">\(F\)</span> in each iteration like the
        vectors is in gradient descent, such that one ends up with a
        final classifier that consists of a sum of these base functions:
        <span class="math inline">\(F=\sum_{t=1}^T h_t\)</span>. The
        base functions <span class="math inline">\(h_t\)</span> most
        used with XGBoost is simple decision trees, but linear functions
        are also available. Three of such trees from the application in
        this article are plotted below.</p>
        <p><img src="/assets/images/2017-11-06-xgboost/treeXGB.jpeg"></p>
        <p>XGBoost utilizes the same idea as normal gradient boosting
        but more like an analog of the Newton-Raphson method in
        functional space and with extra regularizations methods.</p>
        <p>In XGBoost the regularized loss funtion is minimized:</p>
        <p><span class="math display">\[
            L(F_T)=\sum_{i=1}^{N}L[y_i,F_T(x_i)]+\sum_{t=1}^{T}\Omega(h_t),
        \quad F_T(x)=\sum_{t=1}^{T}h_t(x)
        \]</span></p>
        <p>Where the extra sum over <span
        class="math inline">\(\Omega(h_t)\)</span> is regularization
        applied to each tree. Assume that a given tree <span
        class="math inline">\(h_t\)</span> has <span
        class="math inline">\(J_t\)</span> terminal nodes and weights
        <span class="math inline">\(w_{j,t}\)</span> in each leaf node.
        Then this regularization can be expressed as:</p>
        <p><span class="math display">\[
            \Omega(h_t) = \gamma J_t +
        \frac{1}{2}\lambda\sum_{j=1}^{J_t}w_{j,t}^2 + \alpha
        \sum_{j=1}^{J_t} \vert w_{j,t}\vert
        \]</span></p>
        <p>From this one sees that the <span
        class="math inline">\(\gamma\)</span> work as a minimal loss
        reduction required to make a new split in a terminal node, in an
        arbitrary iteration of the algorithm. The sums <span
        class="math inline">\(\alpha\sum_{j=1}^{J_t} \vert
        w_{j,t}\vert\)</span> and <span
        class="math inline">\(\frac{1}{2}\lambda\sum_{j=1}^{J_t}w_{j,t}^2\)</span>
        is over the weights in the leaf nodes <span
        class="math inline">\(w_{j,t}\)</span> in tree <span
        class="math inline">\(t\)</span>’th tree. It is <span
        class="math inline">\(L^1\)</span> and <span
        class="math inline">\(L^2\)</span> regularization over the
        weights in the leaf nodes for the <span
        class="math inline">\(t\)</span>’th tree.</p>
        <p>Besides these regularizations XGBoost also gives one the
        opportunity to use subsample a fraction of the dataset in each
        iteration, analogous to stochastic gradient descent, and to use
        a subsample of the columns in each iteration, like a Random
        Forest.</p>
        <p>If you want to read about it I recommend you read this <a
        href="https://arxiv.org/abs/1603.02754">Chen and Guestrin
        [2016]</a> article from the guys who made XGBoost, I can’t say I
        understand the implementation they describe in the article but
        this is also a great part of XGboosts succes, besides the
        mathematical model.</p>
        <h2 id="model-tuning-and-validation">Model tuning and
        Validation</h2>
        <p>XGBoost consistst of a lot of parameters. The ones I have
        used most succesfully for regression problems are listed below,
        many of them with their default value. To see a list of them all
        go to <a
        href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.md">XGBoost
        parameters</a>. Besides these I also highly recommend you test
        the <code>scale_pos_weight</code> parameter if you have an
        imbalanced classification problem.</p>
        <div class="sourceCode" id="cb5"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>param <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="st">&quot;booster&quot;</span>             <span class="ot">=</span> <span class="st">&quot;gbtree&quot;</span>, <span class="co"># specifying that the basis functions are regression trees</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;eta&quot;</span>                 <span class="ot">=</span> <span class="fl">0.05</span>,     <span class="co"># specifying the learning rate 0&lt;eta&lt;1</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;gamma&quot;</span>               <span class="ot">=</span> <span class="dv">50</span>,       <span class="co"># specifying loss for extra leaf node</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;max_depth&quot;</span>           <span class="ot">=</span> <span class="dv">5</span>,        <span class="co"># maximum depth of tree</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;alpha&quot;</span>               <span class="ot">=</span> <span class="dv">0</span>,        <span class="co"># L1 regularization</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;lambda&quot;</span>              <span class="ot">=</span> <span class="dv">1</span>,        <span class="co"># L2 regularization</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;subsample&quot;</span>           <span class="ot">=</span> <span class="fl">0.5</span>,      <span class="co"># how much of the data is used in subsampling for a new iteration(rows)</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;colsample_bytree&quot;</span>    <span class="ot">=</span> <span class="fl">0.8</span>       <span class="co"># how much of the data is used in subsampling for a new iteration(collumns)</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
        <p>Now you can ask yourself: What to do with all these fucking
        parameters? Answer: Validate the model for different values of
        the parameters, each in a routine that gives you an
        approximation for generalization performance, i.e. approximates
        how well the algorithm will perform on new unseen data like <a
        href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Cross_validation_for_time-series_models">cross-validation</a>
        for stationary data or a <a
        href="https://en.wikipedia.org/wiki/Backtesting">backtest</a>
        for financial data, or another routine for the given problem.
        Most commonly each parameter is tested in a grid search, random
        search or by Bayesian search. In my experience the random search
        works very well and is easily implemented, when implemented you
        can just start it, and let it run over the weekend, and then
        analyze the results Monday when you get back to work.</p>
        <p>One of the routines mentioned above are implemented in the
        <code>xgboost</code> package, a cross-validation, and
        implemented in R below.</p>
        <div class="sourceCode" id="cb6"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>cvData <span class="ot">&lt;-</span> <span class="fu">xgb.cv</span>(<span class="at">params =</span> param,</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                 xg.labelledData,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">nrounds =</span> <span class="dv">2000</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">verbose =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">nfold =</span> <span class="dv">3</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;mae&quot;</span>))</span></code></pre></div>
        <p>Eventhough it is already implemented I recommend, if you have
        the time, that you instead make your own grid-search or other
        routine, s.t. you know that all the algorithms you are
        validating are compared consistently, and in a routine you
        believe approximates the generalization error.</p>
        <p>But for this post we will just use the above routine. As a
        output of the routine we can plot the validation error (the
        estimated generalization error from the cross-validation
        routine) together with the training error. We see that the model
        starts to overfit when we get more and more trees (the
        difference between the validation error and training error
        increases). But the minimum validation error is at 2000 trees so
        lets use that.</p>
        <p><img src="/assets/images/2017-11-06-xgboost/fittingXGB.jpg"></p>
        <p>Now we train our model using the below code.</p>
        <div class="sourceCode" id="cb7"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>bst <span class="ot">&lt;-</span> <span class="fu">xgboost</span>(<span class="at">params =</span> param,</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>              xg.labelledData,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">nrounds =</span> <span class="dv">2000</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">verbose =</span> <span class="cn">TRUE</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
        <h2 id="model-inspection">Model inspection</h2>
        <p>First we will plot the feature importance. Feature importance
        is calculated by summing over the internal nodes in the tree. In
        each of these nodes one of the features is applied to split the
        constant region in two subregions. The way this split is chosen
        is by the maximum improvement in loss over the constant fit in
        the region. Feature importance is now the sum over improvement
        in loss over the internal nodes for a given feature averaged
        over all the trees in the sum <span
        class="math inline">\(\sum_{t=1}^T h_t\)</span>.</p>
        <div class="sourceCode" id="cb8"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>importance_matrix <span class="ot">=</span> <span class="fu">xgb.importance</span>(names, <span class="at">model =</span> bst)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">xgb.ggplot.importance</span>(importance_matrix, <span class="at">top_n =</span> <span class="dv">20</span>, <span class="at">n_clusters =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span></code></pre></div>
        <p><img src="/assets/images/2017-11-06-xgboost/importanceXGB.jpg"></p>
        <p>After one has identified the most important variables in the
        model, it is often of interest to understand the dependence of
        the prediction on the joint feature values. A plot of the
        prediction as a function of its inputs is however restricted to
        a functional relationship with just two inputs (<span
        class="math inline">\(d\leq 2\)</span>), a 3-d plot. If one
        wishes to plot a model with more inputs than two one has to get
        creative. A smart alternative is to approximate the prediction
        as a function of a subset of the inputs, s.t. partial effects
        can be plotted in a 2-d or 3-d plot in a model with more than
        two inputs. This is what one does in partial dependence plots.
        Below is an example of such a plot, which yield a plausible
        partial effect of an increase in OverallQual of the house, for
        this model.</p>
        <p><img src="/assets/images/2017-11-06-xgboost/partialXGB.jpg"></p>
        <p>Mathematically you can see a partial dependence plot like it
        like this: Assume <span class="math inline">\(X_S\)</span> is
        the partial random variables (a random vector) you wish to
        compute the partial effects of, you then just “average out” the
        other variables, the <span class="math inline">\(X_C\)</span>
        random variables, using the observed values <span
        class="math inline">\(x_{Ci}\)</span>, which gives you the
        below. <span
        class="math display">\[  \tilde{F}_S(X_S)=\frac{1}{N}\sum_{i=1}^{N}F(X_S,x_{iC})\]</span>
        Below you see the code how to do this in R.</p>
        <div class="sourceCode" id="cb9"><pre
        class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make partial predictions</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(grid_i <span class="cf">in</span> grid){</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># constructing temporary data-frame</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  train_tmp <span class="ot">&lt;-</span> train</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  train_tmp[,mostImpFeat] <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>,<span class="fu">dim</span>(train_tmp)[<span class="dv">1</span>]) <span class="sc">*</span> grid_i</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># names of the regressors and the one-hot encoded factors</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  xgb.train_tmp <span class="ot">&lt;-</span> <span class="fu">sparse.model.matrix</span>(<span class="sc">~</span> . <span class="sc">-</span><span class="dv">1</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                                       <span class="fu">model.frame</span>(train_tmp[,<span class="sc">-</span><span class="fu">which</span>(<span class="fu">names</span>(train_tmp) <span class="sc">%in%</span> <span class="st">&quot;SalePrice&quot;</span>)],<span class="at">na.action =</span> na.pass),</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>                                       <span class="at">contrasts.arg =</span> <span class="fu">lapply</span>(train_tmp[,<span class="fu">sapply</span>(train_tmp,is.factor)], contrasts, <span class="at">contrasts=</span><span class="cn">FALSE</span>))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  partialPredictions <span class="ot">&lt;-</span> <span class="fu">c</span>(partialPredictions, <span class="fu">mean</span>(<span class="fu">predict</span>(bst, <span class="at">newdata =</span> xgb.train_tmp)))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
        <p>This estimate can be heavy in calculation if the dataset is
        big, an easy fix is just to approximate it using a subsample of
        the observed sample, which still approximates the same partial
        relationship. It should be noted that this approximation method
        is not restricted to weighted sums of trees, but can be applied
        to any “Black Box” models. ## Conclusion This post introduced
        the XGBoost algorithm, which is used a lot on the Kaggle
        platform and in practice. It also introduced some model
        diagnostics for XGBoost, one of these being the partial
        dependence plot which is a nice general method to vizualize the
        effects of a black-box method in a 1D- or 2D-plot.</p>
        <h2 id="references">References</h2>
        <p>[1] Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree
        Boosting System. Discovery and Data Mining - KDD ’16, 785-794,
        2016.</p>
        <p>[2] <a href="https://github.com/dmlc/xgboost">Github
        Repo</a></p>
        <br>
        <br>

        <!--- Disqus --->
        <div id="disqus_thread"></div>
        <script>
            /**
            *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
            /*
           var disqus_config = function () {
           this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
           this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
           };
                 */
            (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://machinelearningnotes-1.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>        
    </div>


    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>    
  </body>
</html>
