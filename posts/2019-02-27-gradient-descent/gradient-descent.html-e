<!doctype html>
<html lang="en">
  <head>

      <style>
        img {
          max-width: 100%;
          margin: auto;
          display: block;
        }
        figcaption {
          max-width: 100%;
          margin: auto;
          display: block;
        }
        code {
          font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
          background-color: #f5f5f5;
          padding: .2em .4em;
          font-size: 85%;
          margin: 0;
        }
        pre {
          margin: 1em 0;
          background-color: #f5f5f5;
          padding: 1em;
          overflow: auto;
        }
        pre code {
          padding: 0;
          overflow: visible;
          overflow-wrap: normal;
        }
        .sourceCode {
         background-color: #f5f5f5;
         overflow: visible;
        }
        hr {
          background-color: #1a1a1a;
          border: none;
          height: 1px;
          margin: 1em 0;
        }
        table {
          margin: 1em 0;
          border-collapse: collapse;
          width: 100%;
          overflow-x: auto;
          display: block;
          font-variant-numeric: lining-nums tabular-nums;
        }
        table caption {
          margin-bottom: 0.75em;
        }
        tbody {
          margin-top: 0.5em;
          border-top: 1px solid #1a1a1a;
          border-bottom: 1px solid #1a1a1a;
        }
        th {
          border-top: 1px solid #1a1a1a;
          padding: 0.25em 0.5em 0.25em 0.5em;
        }
        td {
          padding: 0.125em 0.5em 0.25em 0.5em;
        }
        header {
          margin-bottom: 4em;
          text-align: center;
        }
        #TOC li {
          list-style: none;
        }
        #TOC ul {
          padding-left: 1.3em;
        }
        #TOC > ul {
          padding-left: 0;
        }
        #TOC a:not(:hover) {
          text-decoration: none;
        }
        code{white-space: pre-wrap;}
        span.smallcaps{font-variant: small-caps;}
        span.underline{text-decoration: underline;}
        div.column{display: inline-block; vertical-align: top; width: 50%;}
        div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
        ul.task-list{list-style: none;}
        pre > code.sourceCode { white-space: pre; position: relative; }
        pre > code.sourceCode > span { line-height: 1.25; }
        pre > code.sourceCode > span:empty { height: 1.2em; }
        .sourceCode { overflow: visible; }
        code.sourceCode > span { color: inherit; text-decoration: inherit; }
        div.sourceCode { margin: 1em 0; }
        pre.sourceCode { margin: 0; }
        @media screen {
        div.sourceCode { overflow: auto; }
        }
        @media print {
        pre > code.sourceCode { white-space: pre-wrap; }
        pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
        }
        pre.numberSource code
          { counter-reset: source-line 0; }
        pre.numberSource code > span
          { position: relative; left: -4em; counter-increment: source-line; }
        pre.numberSource code > span > a:first-child::before
          { content: counter(source-line);
            position: relative; left: -1em; text-align: right; vertical-align: baseline;
            border: none; display: inline-block;
            -webkit-touch-callout: none; -webkit-user-select: none;
            -khtml-user-select: none; -moz-user-select: none;
            -ms-user-select: none; user-select: none;
            padding: 0 4px; width: 4em;
            color: #aaaaaa;
          }
        pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
        div.sourceCode
          {   }
        @media screen {
        pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
        }
        code span.al { color: #ff0000; font-weight: bold; } /* Alert */
        code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
        code span.at { color: #7d9029; } /* Attribute */
        code span.bn { color: #40a070; } /* BaseN */
        code span.bu { } /* BuiltIn */
        code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
        code span.ch { color: #4070a0; } /* Char */
        code span.cn { color: #880000; } /* Constant */
        code span.co { color: #60a0b0; font-style: italic; } /* Comment */
        code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
        code span.do { color: #ba2121; font-style: italic; } /* Documentation */
        code span.dt { color: #902000; } /* DataType */
        code span.dv { color: #40a070; } /* DecVal */
        code span.er { color: #ff0000; font-weight: bold; } /* Error */
        code span.ex { } /* Extension */
        code span.fl { color: #40a070; } /* Float */
        code span.fu { color: #06287e; } /* Function */
        code span.im { color: #007020; font-weight: bold; } /* Import */
        code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
        code span.kw { color: #007020; font-weight: bold; } /* Keyword */
        code span.op { color: #666666; } /* Operator */
        code span.ot { color: #007020; } /* Other */
        code span.pp { color: #bc7a00; } /* Preprocessor */
        code span.sc { color: #4070a0; } /* SpecialChar */
        code span.ss { color: #bb6688; } /* SpecialString */
        code span.st { color: #4070a0; } /* String */
        code span.va { color: #19177c; } /* Variable */
        code span.vs { color: #4070a0; } /* VerbatimString */
        code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
      </style>
              <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
      
                <!-- Using Bootstrap starter template: https://getbootstrap.com/docs/4.0/getting-started/introduction/-->
                <!-- Required meta tags -->
                <meta charset="utf-8">
                <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

                <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png">
                <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
                <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
                <link rel="manifest" href="/assets/favicon/site.webmanifest">

                <!-- Font Awesome CSS -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

                <!-- Disqus -->
                <script id="dsq-count-scr" src="//machinelearningnotes-1.disqus.com/count.js" async></script>
                      <script
                      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
                      type="text/javascript"></script>
      
    <title>Gradient Descent</title>
  </head>
  <body>

        <nav class="navbar navbar-expand-lg navbar-light bg-light">
            <a class="navbar-brand" href="/">ML-Notes</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
                <div class="navbar-nav">
                    <a class="nav-link" href="/index.html"> Notes </a>
                    <a class="nav-link" href="/courses.html"> Courses </a>
                    <a class="nav-link" href="/about.html"> About </a>
                </div>
            </div>
        </nav>

    
    <div class="container">
        <br>
        <div class="page-header">
            <div class="pull-left">
                <h1>Gradient Descent</h1>
                <strong>Algorithm</strong>
                <div class="text-muted">2019-02-27</div>
            </div>
            
            <div class="pull-right">

                                <div class="text-right">
                    <a href="https://github.com/laegsgaardTroels/laegsgaardTroels.github.io/tree/master/src/posts/2019-02-27-gradient-descent">
                        [<i>View source</i> <span class="fa fa-github"></span>]
                    </a>
                </div>
                
                
            </div>
            <div class="clearfix"></div>
        </div>
        

        <br>
        <div class="cell raw">

        </div>
        <div class="cell markdown">
        <p>The gradient descent algorithm and its variants are some of
        the most widely used optimization algorithms in machine learning
        today. In this post a super simple example of gradient descent
        will be implemented.<!--more--></p>
        </div>
        <section id="example" class="cell markdown">
        <h2>Example</h2>
        <p>We will use the simple function <span
        class="math inline">\(L(x)=x^2\)</span>, and call it our
        <em>loss function</em>.</p>
        </section>
        <div class="cell code" data-execution_count="1">
        <div class="sourceCode" id="cb1"><pre
        class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The maximum and minimum value of the function.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The loss function.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="kw">lambda</span> x: x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(<span class="dv">0</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="op">-</span>M, M)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, L(theta))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.title(<span class="st">&#39;Loss function&#39;</span>, size<span class="op">=</span><span class="dv">20</span>)</span></code></pre></div>
        <div class="output display_data">
        <p><img
        src="posts/2019-02-27-gradient-descent/56edc4e41910aa52557f03796e350bb6b8e4636e.png" /></p>
        </div>
        </div>
        <section id="algorithm" class="cell markdown">
        <h2>Algorithm</h2>
        <p>The derivative of <span
        class="math inline">\(L(\theta)=\theta^2\)</span> wrt. x is
        <span class="math inline">\(\frac{\partial}{\partial
        \theta}L(\theta)=2\theta\)</span>. The gradient descent
        algorithm goes as follow:</p>
        <ol>
        <li><p><strong>Initiate</strong> <span
        class="math inline">\(\theta_0\in\mathbb{R},
        \eta\in\mathbb{R}\)</span>.</p>
        <p>1.1. <strong>Update</strong>: <span
        class="math inline">\(\theta_{t+1} = \theta_t - \eta
        \frac{\partial}{\partial \theta} L(\theta_t)\)</span>.</p>
        <p>1.2. <strong>Stop</strong>: If stopping criterium is
        satisfied.</p></li>
        </ol>
        <p>Intuitively one takes a small step in the direction of
        steepest local descent. Lets implement some helper functions to
        simulate the algorithm.</p>
        </section>
        <div class="cell code" data-execution_count="16">
        <div class="sourceCode" id="cb2"><pre
        class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_gradient_descent(theta_0, eta, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Simulate the next gradient by running the algorithm.</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">        theta_0 (float): The initial value of theta.</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">        eta (float): The learning rate.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        max_iter (int): The maximum number of iterations.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        List[float]: A list of thetas walked by the algorithm.</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the results in below</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    thetas <span class="op">=</span> np.zeros(max_iter <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The derivative of x**2</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    dL <span class="op">=</span> <span class="kw">lambda</span> x : <span class="dv">2</span> <span class="op">*</span> x</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Initiate</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    thetas[<span class="dv">0</span>] <span class="op">=</span> M</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1.1. Update</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        thetas[t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> thetas[t] <span class="op">-</span> eta <span class="op">*</span> dL(thetas[t])</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> thetas</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot(theta_path, eta):</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Plot the path walked by the algorithm.</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co">        theta_path (List[float]): A list of thetas walked by the algorithm.</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="co">        eta (float): The learning rate.</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    plt.plot(theta, L(theta))</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    plt.plot(theta_path, L(theta_path), label<span class="op">=</span>eta)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    plt.legend(prop<span class="op">=</span>{<span class="st">&#39;size&#39;</span>: <span class="dv">15</span>})</span></code></pre></div>
        </div>
        <section id="results" class="cell markdown">
        <h2>Results</h2>
        <p>Lets plot the path of the gradient descent algorithm.</p>
        </section>
        <div class="cell code" data-execution_count="17">
        <div class="sourceCode" id="cb3"><pre
        class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>theta_path <span class="op">=</span> simulate_gradient_descent(M, eta)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">&#39;text&#39;</span>, usetex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plot(theta_path, eta)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.text(theta_path[<span class="dv">0</span>], L(theta_path[<span class="dv">0</span>]), <span class="vs">r&#39;$\theta_0$&#39;</span>, size <span class="op">=</span> <span class="dv">20</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.text(theta_path[<span class="op">-</span><span class="dv">1</span>], L(theta_path[<span class="op">-</span><span class="dv">1</span>]), <span class="vs">r&#39;$\theta_T$&#39;</span>, size <span class="op">=</span> <span class="dv">20</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
        <div class="output display_data">
        <p><img
        src="posts/2019-02-27-gradient-descent/8dde943c4d7fc7cd9c92b42786c1fa0cf614dce5.png" /></p>
        </div>
        </div>
        <div class="cell markdown">
        <p>It takes small steps with small <span
        class="math inline">\(\eta\)</span> and large steps with large
        <span class="math inline">\(\eta\)</span>.</p>
        </div>
        <div class="cell code" data-execution_count="18">
        <div class="sourceCode" id="cb4"><pre
        class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, eta <span class="kw">in</span> <span class="bu">enumerate</span>(np.arange(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.05</span>)):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">4</span>, <span class="dv">5</span>, idx <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    eta <span class="op">=</span> <span class="bu">round</span>(eta, <span class="dv">3</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    theta_path <span class="op">=</span> simulate_gradient_descent(M, eta)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    plot(theta_path, eta)</span></code></pre></div>
        <div class="output display_data">
        <p><img
        src="posts/2019-02-27-gradient-descent/735d3e6045507aa49c4810c72777b63d3bbfd9a3.png" /></p>
        </div>
        </div>
        <div class="cell markdown">
        <p>With <span class="math inline">\(\eta&gt;1\)</span> it
        occilates to infinity.</p>
        </div>
        <div class="cell code" data-execution_count="19">
        <div class="sourceCode" id="cb5"><pre
        class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">1.01</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>theta_path <span class="op">=</span> simulate_gradient_descent(M, eta)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plot(theta_path, eta)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.text(theta_path[<span class="dv">0</span>], L(theta_path[<span class="dv">0</span>]), <span class="vs">r&#39;$\theta_0$&#39;</span>, size <span class="op">=</span> <span class="dv">20</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.text(theta_path[<span class="op">-</span><span class="dv">1</span>], L(theta_path[<span class="op">-</span><span class="dv">1</span>]), <span class="vs">r&#39;$\theta_T$&#39;</span>, size <span class="op">=</span> <span class="dv">20</span>)</span></code></pre></div>
        <div class="output display_data">
        <p><img
        src="posts/2019-02-27-gradient-descent/77c37df0108f154025a98a6ed13ef21018856cfd.png" /></p>
        </div>
        </div>
        <section id="intuition" class="cell markdown">
        <h2>Intuition</h2>
        <p>The intuition of gradient descent is that in each iteration
        of the <strong>update</strong> step one does a local linear
        approximation of the loss function, a 1st order Taylor expansion
        of <span class="math inline">\(L(\theta+d)\)</span> around <span
        class="math inline">\(d=0\)</span>, that is for <span
        class="math inline">\(d\in\mathbb{R},
        \theta_t\in\mathbb{R}\)</span></p>
        <p><span class="math display">\[
        \begin{align}
            L(\theta_t+d) &amp;\approx L(\theta_t) + \frac{\partial
        L(\theta_t+d)}{\partial d}(0) \cdot (d - 0) \\
            &amp;= L(\theta_t) + \frac{\partial (\theta_t + d)}{\partial
        d}(0) \frac{\partial L(x)}{\partial x}(\theta_t+0) \cdot (d -
        0)  &amp;&amp; \text{Chain rule.} \\
            &amp;= L(\theta_t) + \frac{\partial L(x)}{\partial x}
        (\theta_t) \cdot d
        \end{align}
        \]</span></p>
        <p>Using this local approximation one sees that <span
        class="math inline">\(L\)</span> is minimized in the direction
        of the negative gradient <span
        class="math inline">\(d=-\frac{\partial L(x)}{\partial x}
        (\theta_t)\)</span>.</p>
        <p>The reason why the algorithm has the stepsize <span
        class="math inline">\(\eta\)</span> is intuitvely because the
        local linear approximation only works well around a
        neighbourhood of <span
        class="math inline">\(\theta_t\)</span>.</p>
        </section>
        <section id="higher-dimensions" class="cell markdown">
        <h2>Higher Dimensions</h2>
        </section>
        <div class="cell markdown">
        <p>This also holds in higher dimensions where the approximation
        is <span class="math inline">\(d\in\mathbb{R^d},
        \theta_t\in\mathbb{R^d}\)</span></p>
        <p><span class="math display">\[
        \begin{align}
            L(\theta_t+d) \approx L(\theta_t) + \nabla L(\theta_t)^T d
        \end{align}
        \]</span></p>
        <p>Restricting <span class="math inline">\(\lVert
        d\rVert=1\)</span> then one can see by Cauchy-Schwarz inequality
        that</p>
        <p><span class="math display">\[
        \begin{align}
            \nabla L(\theta_t)^T d &amp; \geq - \lvert \nabla
        L(\theta_t)^T d \rvert \\
            &amp; \geq - \lVert \nabla L(\theta_t) \rVert \lVert d
        \rVert &amp;&amp; \text{Cauchy Schwarz.} \\
                &amp;= \nabla L(\theta_t)^T\frac{- \nabla
        L(\theta_t)}{\lVert\nabla L(\theta_t)\rVert}
        \end{align}
        \]</span></p>
        <p>So <span class="math inline">\(\nabla L(\theta_t)^T d \geq
        \nabla L(\theta_t)^T d^\star\)</span> where <span
        class="math inline">\(d^\star=\frac{-\nabla
        L(\theta_t)}{\lVert\nabla L(\theta_t)\rVert}\)</span>, therefore
        <span class="math inline">\(-\nabla L(\theta_t)\)</span> is the
        local direction which minimizes the loss function.</p>
        <p>Gradient descent applies this local approximation and moves
        along the negative gradient in each iteration.</p>
        <p><span class="math display">\[
        \theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)
        \]</span></p>
        <p>The <span class="math inline">\(\eta\)</span> is often called
        the <em>learning rate</em>. It is a tuning parameter, that
        controls how far the algorithm steps along the negative
        gradient.</p>
        </div>
        <br>
        <br>

                <!--- Disqus --->
                <div id="disqus_thread"></div>
                <script>
                    /**
                    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                         *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
                    /*
                   var disqus_config = function () {
                   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
                   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                   };
                         */
                    (function() { // DON'T EDIT BELOW THIS LINE
                            var d = document, s = d.createElement('script');
                            s.src = 'https://machinelearningnotes-1.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>                
    </div>


    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>    
  </body>
</html>
