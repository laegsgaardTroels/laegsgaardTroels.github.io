<!doctype html>
<html lang="en">
  <head>

      <style>
        img {
          max-width: 100%;
          margin: auto;
          display: block;
        }
        figcaption {
          max-width: 100%;
          margin: auto;
          display: block;
        }
        code {
          font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
          background-color: #f5f5f5;
          padding: .2em .4em;
          font-size: 85%;
          margin: 0;
        }
        pre {
          margin: 1em 0;
          background-color: #f5f5f5;
          padding: 1em;
          overflow: auto;
        }
        pre code {
          padding: 0;
          overflow: visible;
          overflow-wrap: normal;
        }
        .sourceCode {
         background-color: #f5f5f5;
         overflow: visible;
        }
        hr {
          background-color: #1a1a1a;
          border: none;
          height: 1px;
          margin: 1em 0;
        }
        table {
          margin: 1em 0;
          border-collapse: collapse;
          width: 100%;
          overflow-x: auto;
          display: block;
          font-variant-numeric: lining-nums tabular-nums;
        }
        table caption {
          margin-bottom: 0.75em;
        }
        tbody {
          margin-top: 0.5em;
          border-top: 1px solid #1a1a1a;
          border-bottom: 1px solid #1a1a1a;
        }
        th {
          border-top: 1px solid #1a1a1a;
          padding: 0.25em 0.5em 0.25em 0.5em;
        }
        td {
          padding: 0.125em 0.5em 0.25em 0.5em;
        }
        header {
          margin-bottom: 4em;
          text-align: center;
        }
        #TOC li {
          list-style: none;
        }
        #TOC ul {
          padding-left: 1.3em;
        }
        #TOC > ul {
          padding-left: 0;
        }
        #TOC a:not(:hover) {
          text-decoration: none;
        }
        code{white-space: pre-wrap;}
        span.smallcaps{font-variant: small-caps;}
        span.underline{text-decoration: underline;}
        div.column{display: inline-block; vertical-align: top; width: 50%;}
        div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
        ul.task-list{list-style: none;}
      </style>
              <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
      
                <!-- Using Bootstrap starter template: https://getbootstrap.com/docs/4.0/getting-started/introduction/-->
                <!-- Required meta tags -->
                <meta charset="utf-8">
                <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
                <link rel="shortcut icon" type="image/png" href="/assets/images/2017-09-29-adaboost-the-original-boosting-algorithm/AdaBoostExampleData.png">

                <!-- <link rel="stylesheet" href="/assets/css/bootstrap.min.css">Bootstrap CSS -->


                <!-- 
                <script type="text/x-mathjax-config">
                    MathJax.Hub.Config({
                    jax: ["input/TeX", "output/HTML-CSS"],
                    tex2jax: {
                      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
                      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
                      processEscapes: true,
                      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                    }
                    //,
                    //displayAlign: "left",
                    //displayIndent: "2em"
                  });
                </script>
                <script type="text/javascript" async
                        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
                </script>
                MathJax -->

                <!-- Font Awesome CSS -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

                <!-- Disqus -->
                <script id="dsq-count-scr" src="//machinelearningnotes-1.disqus.com/count.js" async></script>
                      <script
                      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
                      type="text/javascript"></script>
      
    <title>Model Evaluation</title>
  </head>
  <body>

        <nav class="navbar navbar-expand-lg navbar-light bg-light">
            <a class="navbar-brand" href="/">ML-Notes</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
                <div class="navbar-nav">
                    <a class="nav-link" href="/index.html"> Blog </a>
                    <a class="nav-link" href="/courses.html"> Courses </a>
                    <a class="nav-link" href="/about.html"> About </a>
                </div>
            </div>
        </nav>

    
    <div class="container">
        <br>
        <div class="page-header">
            <div class="pull-left">
                <h1>Model Evaluation</h1>
                <strong>Statistical Learning</strong>
                <div class="text-muted">2018-11-09</div>
            </div>
            
            <div class="pull-right">

                
            </div>
            <div class="clearfix"></div>
        </div>
        

        <br>
        <p>Giving an estimate of generalization error in Machine
        Learning is vital.<!--more--></p>
        <p>This is to minimize <em>risk</em> given a hypothesis space
        <span class="math inline">\(\mathcal{H}\)</span>. To simplyfy
        matters assume the hypothesis space consists of a finilite set
        of estimators <span
        class="math display">\[\mathcal{H}=\{h_1,h_2,\dots,h_M\}\]</span>.
        Given a <em>loss function</em> <span
        class="math inline">\(L\)</span> which we for simplicity assume
        is the 0-1 loss <span
        class="math display">\[L(\hat{y},y)=1_{\{\hat{y}\neq
        y\}}(\hat{y},y)\]</span>. Risk is then defined as</p>
        <p><span class="math display">\[
        \begin{aligned}
        R(h)&amp;=\mathbb{E}[L(h(X),Y)] \\
        &amp;=\int L(h(x),y)d\mathbb{P}_{X,Y}(dx,dy)
        \end{aligned}
        \]</span></p>
        <p>The main objective is to solve</p>
        <p><span class="math display">\[
        \underset{h\in\mathcal{H}}{\text{argmin}} \ R(h)
        \]</span></p>
        <p>That is, the expected loss over over the hypothesis space. We
        don’t have this distribution <span
        class="math inline">\(\mathbb{P}_{X,Y}\)</span> but we have
        samples from it. We therefore approximate this risk in practice
        using the samples. One way is to use the <em>empirical
        risk</em>:</p>
        <p><span class="math display">\[
        \begin{aligned}
        \hat{R}(h)=\sum_{i=1}^NL(h(x_i),y_i)
        \end{aligned}
        \]</span></p>
        <p>and then try to minimize</p>
        <p><span class="math display">\[
        \underset{h\in\mathcal{H}}{\text{argmin}} \ \hat{R}(h)
        \]</span></p>
        <p>This is known as <em>Empirical Risk Mimimization</em>
        (ERM).</p>
        <h2 id="theory">Theory</h2>
        <p>Lets first go through a <em>learning bound</em> which can be
        used to justify the training validation split given the above
        objective.</p>
        <p><em>Assume <span class="math inline">\(H&lt;\infty\)</span>
        and <span class="math display">\[1_{\{\hat{y}\neq
        y\}}(\hat{y},y)\]</span>, then</em></p>
        <p><span class="math display">\[
        \forall\epsilon&gt;0:\mathbb{P}\left(\max_{h\in\mathcal{H}}\vert
        R(h)-\hat{R}(h)\vert&gt;\epsilon\right)\leq 2\vert
        \mathcal{H}\vert e^{-2N\epsilon^2}
        \]</span></p>
        <p><em>or equivalently</em></p>
        <p><span class="math display">\[
        \forall\delta&gt;0:\mathbb{P}\left(\max_{h\in\mathcal{H}}\vert
        R(h)-\hat{R}(h)\vert &gt;\sqrt{\frac{\log(\vert \mathcal{H}\vert
        )+\log(\frac{2}{\delta})}{2N}}\right)\leq\delta
        \]</span></p>
        <p><em>Proof.</em> Let <span
        class="math inline">\(\epsilon&gt;0\)</span> use the <a
        href="https://en.wikipedia.org/wiki/Boole%27s_inequality">Union
        Bound</a> and <a
        href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding’s
        Inequality</a></p>
        <p><span class="math display">\[
        \begin{aligned}
        \mathbb{P}\left(\max_{h\in\mathcal{H}}\vert R(h)-\hat{R}(h)\vert
        &gt;\epsilon\right)&amp;=\mathbb{P}\left(\bigcup_{h\in\mathcal{H}}\vert
        R(h)-\hat{R}(h)\vert &gt;\epsilon\right)\\
        &amp;\leq\sum_{h\in\mathcal{H}} \mathbb{P}\left(\vert
        R(h)-\hat{R}(h)\vert &gt;\epsilon\right) \\
        &amp;\leq 2\vert \mathcal{H}\vert e^{-2N\epsilon^2}
        \end{aligned}
        \]</span></p>
        <p>Observing that</p>
        <p><span class="math display">\[
        \delta=2\vert \mathcal{H}\vert e^{-2N\epsilon^2}\Leftrightarrow
        \epsilon=\sqrt{\frac{\log(\vert \mathcal{H}\vert
        )+\log(\frac{2}{\delta})}{2N}}
        \]</span></p>
        <p>one sees the equivalent statement. QED.</p>
        <p>The above theorem states a learning bound. If <span
        class="math inline">\(\vert\mathcal{H}\vert&lt;\infty\)</span>
        then with probability less than or equal to <span
        class="math inline">\(1-\delta\)</span></p>
        <p><span class="math display">\[
        \forall
        h\in\mathcal{H}:R(h)\leq\hat{R}(h)+\sqrt{\frac{\log{\vert
        \mathcal{H}\vert +\log\frac{2}{\delta}}}{2N}}
        \]</span></p>
        <h2 id="practice">Practice</h2>
        <p>In practice one usually splits the entire dataset into three
        datasets: A Training dataset which is used for training the
        models, <span class="math inline">\(\sim50\%\)</span> of the
        data, and a validation- and test-dataset, <span
        class="math inline">\(\sim25\%\)</span> each, which is used for
        validating and testing the models.</p>
        <p><img src="/assets/images/2018-11-09-model-evaluation-part-1/train_validation_test.png"></p>
        <p>The models are trained on the training dataset, using an
        algorithm, the performance of this trained model is then
        evaluated by predicting onto the validation dataset. One does
        this for multiple models and then selects the best performing
        one.</p>
        <p><img src="/assets/images/2018-11-09-model-evaluation-part-1/train_validation.png"></p>
        <p>To theoretically justify this approach one can use the above
        generalization bound. Assume we have tried our luck and fitted
        <span class="math inline">\(1\)</span> mio. different models
        <span
        class="math display">\[\mathcal{H}=\{h_1,h_2,\dots,h_{1000000}\}\]</span>
        on the training dataset. These models could consist of KNN for
        different <span class="math inline">\(k\)</span>, classification
        trees for different depths, random forests for different number
        of trees and depths for each tree and so on.</p>
        <p>The above generalization bound gives a worst case performance
        with some probability. Assume we want to be sure that our model
        performs well with a probability of <span
        class="math inline">\(0.95\)</span>, in above <span
        class="math inline">\(\delta=0.05\)</span>, and that we have a
        validation sample of size <span class="math inline">\(1\)</span>
        mio.</p>
        <p>We find a classifier which has an error rate of only <span
        class="math inline">\(0.001\)</span>, how well will it then
        peform with 95% probability on new data? Answer:</p>
        <p><span class="math display">\[
        0.001+\sqrt{\frac{\log 10^6 + \log\frac{2}{0.05}}{2
        \cdot 10^6}} = 0.003539520
        \]</span></p>
        <p>Thats pretty close with a high probability. Varying the size
        on the hypothesis space has only a small effect on the bound
        which is quite remarkable.</p>
        <p><img src="/assets/images/2018-11-09-model-evaluation-part-1/Generalization_bound.png"></p>
        <p>The bound can be stated more loosly as: With probability
        <span class="math inline">\(1-\delta\)</span></p>
        <p><span class="math display">\[
        \begin{aligned}
        R(h)&amp;\leq\hat{R}(h)+O(\sqrt{\frac{\log_2\vert
        \mathcal{H}\vert}{N}})\\
        &amp;=\hat{R}(h)+\Omega(\mathcal{H},N)
        \end{aligned}
        \]</span></p>
        <p>The</p>
        <p><span class="math display">\[
        \log_2\vert\mathcal{H}\vert
        \]</span></p>
        <p>has a nice representation as the number of bits needed to
        represent <span class="math inline">\(\mathcal{H}\)</span>,
        which seems like a natural measure to measure the complexity of
        a finite hypothesis space.</p>
        <p>Similar bound holds on the testing dataset, which is used
        give to the user of the model as an assesment of performance,
        the complexity term <span
        class="math inline">\(\Omega(\mathcal{H},N)\)</span> is then
        much smaller, because one usually only selects one model to test
        in practice: The best one on the validation set.</p>
        <p>Above is some of my interpretations of chapter 1 in [1].</p>
        <h2 id="references">References</h2>
        <p>[1] Foundations of Machine Learning (Adaptive Computation and
        Machine Learning series), 2012. Mehryar Mohri, Afshin
        Rostamizadeh, Ameet Talwalkar<br />
        by Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar.</p>
        <br>
        <br>

        <!--- Disqus --->
        <div id="disqus_thread"></div>
        <script>
            /**
            *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
            /*
           var disqus_config = function () {
           this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
           this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
           };
                 */
            (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://machinelearningnotes-1.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>        
    </div>


    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>    
  </body>
</html>
